{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScxxdXAxxGO7"
      },
      "source": [
        "# dAiv 순환환신경망 특강: 시퀀스 데이터의 압축과 생성 [사칙연산 계산기편]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### For Local User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from platform import system\n",
        "\n",
        "%pip install uv\n",
        "!uv init\n",
        "!uv sync\n",
        "\n",
        "if system() == \"Windows\":\n",
        "    %uv add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
        "else:\n",
        "    %uv add torch torchvision torchaudio\n",
        "\n",
        "%uv add matplotlib tqdm numpy pandas scipy jupyter ipywidgets\n",
        "%uv add git+https://github.com/dAiv-CNU/torchdaiv.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### For Colab User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip add matplotlib tqdm numpy pandas scipy jupyter ipywidgets\n",
        "%pip install git+https://github.com/dAiv-CNU/torchdaiv.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Txg_rPdnw_9u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchdaiv.datasets import FixedLengthCalculatorDataset\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k4w2NErxWLD"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfcfIT6WFmUL",
        "outputId": "7f92372a-107c-4f17-94bf-db32cb9ff594"
      },
      "outputs": [],
      "source": [
        "# 데이터 샘플 생성 및 출력 확인인\n",
        "train_dataset = FixedLengthCalculatorDataset(size=320000, max_length=[3, 4])\n",
        "test_dataset = FixedLengthCalculatorDataset(size=3200, max_length=[3, 4])\n",
        "longer_train_dataset = FixedLengthCalculatorDataset(size=320000, max_length=[5, 4])\n",
        "longer_test_dataset = FixedLengthCalculatorDataset(size=3200, max_length=[5, 4])\n",
        "\n",
        "for i in range(10):\n",
        "    train_dataset.sample(i)\n",
        "print()\n",
        "\n",
        "for i, dt in zip(range(10), train_dataset):\n",
        "    print(i, *dt)\n",
        "\n",
        "print(\"\\n\", \"-\" * 20, \"\\n\")\n",
        "\n",
        "for i in range(10):\n",
        "    longer_train_dataset.sample(i)\n",
        "print()\n",
        "\n",
        "for i, dt in zip(range(10), longer_train_dataset):\n",
        "    print(i, *dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# PRACTICE 1: 분류(N-to-1): 사칙연산 계산기 만들기 (MLP)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCJLzvMXxd9L"
      },
      "source": [
        "---\n",
        "## Model Definition\n",
        "> 상황 가정: 토크나이제이션 없이 단순한 MLP로 길이가 동일한 인풋 데이터로 구현\n",
        "\n",
        "> 한계: 다양한 길이의 인풋이 들어온다면 처리를 어떻게 해야 할까?\n",
        "\n",
        "![MLP](https://miro.medium.com/v2/resize:fit:1400/1*KMmqs1A-PqGTmYUk_MpcDw.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql6zXuSGyWCb"
      },
      "outputs": [],
      "source": [
        "# 배치 생성\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "for i, (data, target) in zip(range(1), train_loader):\n",
        "    print(\"Batch\", i+1)\n",
        "    print(f\">>> Data({len(data)}개):\")\n",
        "    [print(d) for d in data]\n",
        "    print(f\">>> Target({len(target)}개):\")\n",
        "    [print(t) for t in target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNv-Xe3FxhCN"
      },
      "outputs": [],
      "source": [
        "class CalculatorMLP(nn.Module):\n",
        "    def __init__(self, hidden_size=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(7, hidden_size),  # 입력: 3(num1) + 1(opr) + 3(num2) = 7\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size//2, hidden_size//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size//4, 4),  # 출력: 4\n",
        "            nn.Sigmoid()  # 출력값을 0~1 사이로 제한\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB4gWdNDzCd_",
        "outputId": "385a240a-ce10-43f9-c2ec-388d86563e1f"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CalculatorMLP()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcY_G0ffxoQF"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터 설정\n",
        "EPOCHS = 10000\n",
        "LEARNING_RATE = 1e-2, 1e-5\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE[0])\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "efk5xMu3yT4x",
        "outputId": "b3c79605-b908-4be5-b6b7-95c187deb8bf"
      },
      "outputs": [],
      "source": [
        "# 학습 루프\n",
        "train_len = len(train_loader)\n",
        "for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_data, batch_labels in tqdm(train_loader, desc=\"Train\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_data = torch.stack(batch_data, dim=1).float().to(device)\n",
        "        batch_labels = torch.stack(batch_labels, dim=1).float().to(device) * 127  # 0~1 범위를 0~127로 변환\n",
        "\n",
        "        outputs = model(batch_data) * 127  # 모델 출력도 0~127 범위로 변환\n",
        "\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item() / len(batch_data)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"\\rEpoch [{epoch+1}/{EPOCHS}], Loss: {total_loss/train_len}\", end=\"\" if (epoch + 1) % 10 == 0 else \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 추론 루프\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "\n",
        "for batch_data, batch_labels in tqdm(test_loader):\n",
        "    batch_data = torch.stack(batch_data, dim=1).float().to(device)\n",
        "    batch_labels = torch.stack(batch_labels, dim=1).float().to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch_data)\n",
        "\n",
        "    loss = criterion(outputs*127, batch_labels*127) / len(batch_data)\n",
        "    total_loss += loss.item()\n",
        "\n",
        "print(f\"Test Loss: {total_loss/len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def pipeline(model, input_str, device=device):\n",
        "    model.eval()\n",
        "    norm = lambda input_ascii: [x / 127 for x in map(ord, input_ascii)]\n",
        "    input_str = input_str.replace(\" \", \"\\00\")\n",
        "    input_data = torch.tensor(norm(str(input_str))).unsqueeze(0)  # (1, 7)\n",
        "    input_data = (input_data / 127).float().to(device)\n",
        "    return \"\".join(map(chr, torch.floor(model(input_data) * 127).int().squeeze(0).tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(pipeline(model, \" 35+ 12\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# PRACTICE 1-1: RNN으로 사칙연산 계산기를 다시 구현\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Model Definition\n",
        "> Recursive한 인코딩을 통해 인풋 길이가 동일하다는 가정을 삭제 가능\n",
        "\n",
        "> 한계: 그러나 생성 결과의 길이가 값 하나가 아니라면 처리 불가능\n",
        "\n",
        "![RNN Encoder](https://blog.kakaocdn.net/dn/dQIPiW/btrHKcZI8NY/FfecZoTxardfpZGGKzR1oK/img.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 배치 생성\n",
        "LONGER_BATCH_SIZE = 1024\n",
        "\n",
        "longer_train_loader = DataLoader(longer_train_dataset, batch_size=LONGER_BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "longer_test_loader = DataLoader(longer_test_dataset, batch_size=LONGER_BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "for i, (data, target) in zip(range(1), longer_train_loader):\n",
        "    print(\"Batch\", i+1)\n",
        "    print(f\">>> Data({len(data)}개):\")\n",
        "    [print(d) for d in data]\n",
        "    print(f\">>> Target({len(target)}개):\")\n",
        "    [print(t) for t in target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CalculatorRNN(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=64, output_dim=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out_last = out[:, -1, :]  # 마지막 시퀀스 출력만 사용\n",
        "        logits = self.fc(out_last)  # 분류기에 연결\n",
        "        return self.sigmoid(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CalculatorRNN()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 하이퍼파라미터 설정\n",
        "EPOCHS = 10000\n",
        "LEARNING_RATE = 1e-2, 1e-5\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE[0])\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 루프\n",
        "train_len = len(longer_train_loader)\n",
        "for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_data, batch_labels in tqdm(longer_train_loader, desc=\"Train\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_data = torch.stack(batch_data, dim=1).unsqueeze(2).float().to(device)\n",
        "        batch_labels = torch.stack(batch_labels, dim=1).float().to(device) * 127  # 0~1 범위를 0~127로 변환\n",
        "\n",
        "        outputs = model(batch_data) * 127  # 모델 출력도 0~127 범위로 변환\n",
        "\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item() / len(batch_data)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"\\rEpoch [{epoch+1}/{EPOCHS}], Loss: {total_loss/train_len}\", end=\"\" if (epoch + 1) % 10 == 0 else \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 추론 루프\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "\n",
        "for batch_data, batch_labels in tqdm(test_loader):\n",
        "    batch_data = torch.stack(batch_data, dim=1).unsqueeze(2).float().to(device)\n",
        "    batch_labels = torch.stack(batch_labels, dim=1).float().to(device) * 127  # 0~1 범위를 0~127로 변환\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch_data) * 127  # 모델 출력도 0~127 범위로 변환\n",
        "\n",
        "    loss = criterion(outputs, batch_labels) / len(batch_data)\n",
        "    total_loss += loss.item()\n",
        "\n",
        "print(f\"Test Loss: {total_loss/len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def pipeline(model, input_str, device=device):\n",
        "    model.eval()\n",
        "    norm = lambda input_ascii: [x / 127 for x in map(ord, input_ascii)]\n",
        "    input_str = input_str.replace(\" \", \"\")\n",
        "    input_data = torch.tensor(norm(str(input_str))).unsqueeze(0)\n",
        "    input_data = (input_data / 127).unsqueeze(2).float().to(device)\n",
        "    return \"\".join(map(chr, torch.floor(model(input_data) * 127).int().squeeze(0).tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(pipeline(model, \"35+12-10\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# PRACTICE 1-2: RNN Encoder-Decoder로 구성해보기 (직접 해보기)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Model Definition\n",
        "> 실습 1-1에서의 생성 길이 한계를 극복하기 위해 Recursive한 인코딩 뿐만 아니라 Recursive한 디코딩도 수행하도록 모델 코드를 작성해보자.\n",
        "\n",
        "![RNN Seq2Seq](https://blog.kakaocdn.net/dn/LUwms/btszM0Eg9wB/e1fPBEkRWGkkX1fSjYJLMk/img.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CalculatorSeq2SeqRNN(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=64, output_dim=1, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.decoder = ??\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder 단계\n",
        "        out, _ = self.encoder(x)\n",
        "        out_last = out[:, -1, :]  # 마지막 시퀀스 출력만 사용\n",
        "        \n",
        "        # Decoder 단계 (여기서는 간단히 Encoder의 출력을 사용)\n",
        "        ??\n",
        "        \n",
        "        # FC 레이어를 통해 출력 생성\n",
        "        logits = self.fc(out_last)  # 분류기에 연결\n",
        "        return self.sigmoid(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
