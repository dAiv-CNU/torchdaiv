{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-Y63VIHN_d7"
   },
   "source": [
    "# dAiv 순환환신경망 특강: 시퀀스 데이터의 압축과 생성 [한영 번역기편편]\n",
    "\n",
    "이번 특강에서는 한국어 문장을 영어로 번역하는 RNN 시퀀스-투-시퀀스(sequence-to-sequence, seq2seq) 모델을 학습하는 방법을 알아보겠습니다.\n",
    "\n",
    "필요 라이브러리: ``torchtext``, ``spacy`` 를 사용하여 데이터셋을 전처리(preprocess)합니다.\n",
    "> 이 예제에서 torchtext는 huggingface의 tokenizers로 대체하여 구현되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NguLhT-UVTbp"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3k1Gcc9UnEj"
   },
   "source": [
    "### For Local User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import system\n",
    "\n",
    "%pip install uv\n",
    "!uv init\n",
    "!uv sync\n",
    "\n",
    "if system() == \"Windows\":\n",
    "    %uv add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "else:\n",
    "    %uv add torch torchvision torchaudio\n",
    "\n",
    "%uv add matplotlib tqdm numpy pandas scipy jupyter ipywidgets\n",
    "%uv add git+https://github.com/dAiv-CNU/torchdaiv.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Colab User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip add matplotlib tqdm numpy pandas scipy jupyter ipywidgets\n",
    "%pip install git+https://github.com/dAiv-CNU/torchdaiv.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSz4zAE4UnEl",
    "outputId": "7e50767e-1264-469b-cca7-ad89e7c74656"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download ko_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ovJ1BgeUnEl"
   },
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:39:10.184481Z",
     "start_time": "2024-05-28T10:39:03.552181Z"
    },
    "id": "d1ELMsANUnEm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BREW\\Desktop\\marimo_test\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import spacy\n",
    "from torchdaiv import datasets\n",
    "from torchdaiv.lectures.kor_eng_translator import nn\n",
    "from torchdaiv.lectures.kor_eng_translator.util import vocabulary, transforms\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from rich.traceback import install\n",
    "#install(show_locals=True)  # 오류 났을 경우 로컬 변수 보고 싶으면 활성화\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGalXsPEUnEn"
   },
   "source": [
    "---\n",
    "## Text Preprocess (with Spacy)\n",
    "\n",
    "``torchtext`` 에는 언어 변환 모델을 만들 때 쉽게 사용할 수 있는 데이터셋을 만들기 적합한 다양한 도구가 있습니다.\n",
    "이 예제에서는 가공되지 않은 텍스트 문장(raw text sentence)을 토큰화(tokenize)하고, 어휘집(vocabulary)을 만들고,\n",
    "토큰을 텐서로 숫자화(numericalize)하는 방법을 알아보겠습니다.\n",
    "\n",
    "| (다만, torchtext는 2024년 4월 이후 더 이상 업데이트가 진행되지 않는다는 점에 유의해야 합니다.)\n",
    "\n",
    "아래를 실행하여 Spacy 토크나이저가 쓸 한국어와 영어에 대한 데이터를 다운로드 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:39:11.520426Z",
     "start_time": "2024-05-28T10:39:10.186488Z"
    },
    "id": "62wq2Y5KUnEn"
   },
   "outputs": [],
   "source": [
    "# spacy tokenizer 적용\n",
    "ko_tokenizer = vocabulary.load_tokenizer(spacy, \"ko_core_news_sm\")\n",
    "en_tokenizer = vocabulary.load_tokenizer(spacy, \"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:39:12.526032Z",
     "start_time": "2024-05-28T10:39:11.522438Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0lVGporOe4n",
    "outputId": "bde8be7c-8e3d-4073-f959-4a0788fcf1bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 애플이 영국의 스타트업을 10억 달러에 인수하는 것을 알아보고 있다.\n",
      "Tokenized: ['애플', '##이', '영국', '##의', '스타트업', '##을', '10', '##억', '달러', '##에', '인수', '##하', '##는', '것', '##을', '알아보고', '있', '##다', '.']\n",
      "\n",
      "> 애플이 (애플+이) | NOUN dislocated\n",
      "> 영국의 (영국+의) | PROPN nmod\n",
      "> 스타트업을 (스타트업+을) | ADV advcl\n",
      "> 10억 (10+억) | NUM compound\n",
      "> 달러에 (달러+에) | NOUN advcl\n",
      "> 인수하는 (인수+하+는) | VERB acl\n",
      "> 것을 (것+을) | NOUN obj\n",
      "> 알아보고 (알아보고) | AUX ROOT\n",
      "> 있다 (있+다) | AUX aux\n",
      "> . (.) | PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.ko.examples import sentences\n",
    "\n",
    "# 작동 확인\n",
    "doc = spacy.load(\"ko_core_news_sm\")(sentences[0])\n",
    "print(\"Original:\", doc.text)\n",
    "print(\"Tokenized:\", ko_tokenizer(sentences[0]), end=\"\\n\\n\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\">\", token.text, f\"({token.lemma_}) |\", token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZdlvQ1NN_d9"
   },
   "source": [
    "---\n",
    "## Load Dataset\n",
    "using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:39:12.557407Z",
     "start_time": "2024-05-28T10:39:12.528054Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_epDEfsFUnEq",
    "outputId": "23f4b565-caa1-4c4e-bd4f-17ece2f97c12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed.\n",
      "Dataset loaded. 5822 samples loaded.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드 - 아무 처리도 하지 않았을 때\n",
    "# 이번 수업에서는 트레인 데이터셋으로만 사용\n",
    "\n",
    "train_dataset = datasets.AnkiKorEngDataset(\"./data\", split_rate=(1.0, 0.0, 0.0))\n",
    "# valid_dataset = datasets.AnkiKorEngDataset(\"./data\", valid=True, split_rate=(0.5, 0.3, 0.2))\n",
    "# test_dataset = datasets.AnkiKorEngDataset(\"./data\", test=True, split_rate=(0.5, 0.3, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:39:12.566294Z",
     "start_time": "2024-05-28T10:39:12.560415Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avPWVriMUnEq",
    "outputId": "04b3bf53-d188-4483-9ad6-5896544386a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 가. Go.\n",
      "1 안녕. Hi.\n",
      "2 뛰어! Run!\n",
      "3 뛰어. Run.\n",
      "4 누구? Who?\n",
      "5 저건 뭐야? What's that?\n",
      "6 누가 그를 그렸습니까? Who drew it?\n",
      "7 쉬엄쉬엄 일해. Work slowly.\n",
      "8 너 늦었어. You're late.\n",
      "9 넌 내 거야. You're mine.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 형태 확인\n",
    "sample = list(zip(*train_dataset[0:5]))+list(zip(*train_dataset[500:505]))\n",
    "for i, (kor, eng) in enumerate(sample):\n",
    "    print(i, kor, eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X44GDLQZUnEr"
   },
   "source": [
    "#### Vocabulary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:40:13.825077Z",
     "start_time": "2024-05-28T10:39:12.568305Z"
    },
    "id": "4Xm_B394UnEr"
   },
   "outputs": [],
   "source": [
    "ko_vocab = vocabulary.build_vocab(raw_dataset=train_dataset.raw_kor, tokenizer=ko_tokenizer)\n",
    "en_vocab = vocabulary.build_vocab(raw_dataset=train_dataset.raw_eng, tokenizer=en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCObu7HfUnEr"
   },
   "source": [
    "#### Convert To Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:41:12.022229Z",
     "start_time": "2024-05-28T10:40:13.827103Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IC8qZlsaUnEs",
    "outputId": "2dd3b6a3-33ac-43e1-8b6e-7e2cd0b3e8a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Special Tokens - PAD_IDX: 0, UNK_IDX: 1\n",
      "Using Special Tokens - PAD_IDX: 0, UNK_IDX: 1\n"
     ]
    }
   ],
   "source": [
    "# 사전 데이터를 기반으로 데이터셋을 텐서로 변환\n",
    "to_tensor = (\n",
    "    transforms.to_tensor(ko_vocab, tokenizer=ko_tokenizer),\n",
    "    transforms.to_tensor(en_vocab, tokenizer=en_tokenizer)\n",
    ")\n",
    "\n",
    "train_dataset.transform(transform=to_tensor)\n",
    "# valid_dataset.transform(transform=to_tensor)\n",
    "# test_dataset.transform(transform=to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:41:12.039158Z",
     "start_time": "2024-05-28T10:41:12.023803Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtxT1KmhUnEs",
    "outputId": "3ebb55eb-f74f-4ac0-f5ea-88a1ca87b9d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([53,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0]) tensor([25,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0])\n",
      "1 tensor([576,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]) tensor([1465,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0])\n",
      "2 tensor([862,   7,  54,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]) tensor([208,  40,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "3 tensor([862,   7,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]) tensor([208,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "4 tensor([94, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0]) tensor([68, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0])\n",
      "5 tensor([2391,  308,   11,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0]) tensor([30,  5, 16, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0])\n",
      "6 tensor([  94,   16,   24,   21, 1105,  322,  525,   11,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0]) tensor([  68, 1092,   17,   10,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0])\n",
      "7 tensor([ 481, 2392, 2393,  578,    4,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0]) tensor([ 58, 725,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "8 tensor([ 38, 864,   7,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]) tensor([  8,   5, 153,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "9 tensor([38, 12, 28, 49,  6, 29,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0]) tensor([  8,   5, 362,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 형태 확인\n",
    "sample = list(zip(*train_dataset[0:5]))+list(zip(*train_dataset[500:505]))\n",
    "for i, (kor, eng) in enumerate(sample):\n",
    "    print(i, kor, eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fWX1KCdUnEs"
   },
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:41:12.046449Z",
     "start_time": "2024-05-28T10:41:12.040172Z"
    },
    "id": "ZCWcsWCDUnEt"
   },
   "outputs": [],
   "source": [
    "# 배치 크기 결정 후 데이터 로더 생성\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# valid_dataload = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset)//20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PRACTICE 2: (N-to-N(or M)): 한영 번역기 (RNN, GRU)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di-0TovtUnEt"
   },
   "source": [
    "---\n",
    "## Model Definition\n",
    "> 인코더 역할을 수행하는 RNN/GRU 하나만 사용하였던 실습 1과는 달리, 레이어를 여러 층으로 쌓는 방식의 Encoder to Decoder 모델을 사용\n",
    "\n",
    "> 인풋과 아웃풋의 길이가 정해져 있는 경우가 아니라면 어떻게 해야할 지에 대한 해결책!\n",
    "\n",
    "![RNN 구조](https://jiho-ml.com/content/images/2020/04/figure3-2.png)\n",
    "\n",
    "> `인코더`는 `한국어를 해석`하고, `디코더`는 `영어를 생성`하는 방식으로 역할을 나눠서 번역을 수행\n",
    "\n",
    "> -> 인코더에서 정보를 압축하고, 디코더가 원하는 크기로 압축을 해제\n",
    "\n",
    "\n",
    "> 참고사항:\n",
    ">> 아래 예시 모델은 공부하기 쉬운 단순한 모델로 번역에 있어 매우 뛰어난 성능을 보이는 모델은 아닙니다.\n",
    ">> 최신 기술 트렌드는 Transformers를 사용하는 것입니다.\n",
    ">> 혹시 관심이 있다면 [Transformer 레이어](https://pytorch.org/docs/stable/nn.html#transformer-layers)에 대해 더 알아보시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:41:12.060074Z",
     "start_time": "2024-05-28T10:41:12.047457Z"
    },
    "id": "HirHms1RUnEt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5499 2525 2525\n"
     ]
    }
   ],
   "source": [
    "# 사전(보캡) 등록\n",
    "nn.set_vocabulary(ko_vocab, en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:41:12.067784Z",
     "start_time": "2024-05-28T10:41:12.061083Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, height=2, hidden=128):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        # 인코더와 디코더를 정의합니다.\n",
    "        self.encoder = nn.Encoder(nn.GRU, height=height, hidden=hidden, dropout=0.3)  # nn.RNN으로도 바꿔보세요.\n",
    "        self.decoder = nn.Decoder(nn.GRU, height=height, hidden=hidden, dropout=0.3)  # nn.RNN으로도 바꿔보세요.\n",
    "\n",
    "    def forward(self, korean, english):\n",
    "        context_vector = self.encoder(korean)\n",
    "        output = self.decoder(english, context_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:41:12.075079Z",
     "start_time": "2024-05-28T10:41:12.068799Z"
    },
    "id": "K5a2LEhXUnEv"
   },
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "epoch = 20\n",
    "lr = 1e-4  # learning rate\n",
    "height = 2\n",
    "hidden = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:41:12.117047Z",
     "start_time": "2024-05-28T10:41:12.076091Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdgjcs7LUnEv",
    "outputId": "855fd0f8-ef58-4563-d390-da7e79617286"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(5499, 256)\n",
       "    (model): GRU(256, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2525, 256)\n",
       "    (model): GRU(256, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
       "    (fc1): Linear(in_features=128, out_features=1262, bias=True)\n",
       "    (fc2): Linear(in_features=1262, out_features=2525, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = Seq2Seq(height=height, hidden=hidden)\n",
    "model.init_optimizer(lr=lr)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:52:22.437958Z",
     "start_time": "2024-05-28T10:52:21.796353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 가능한 경우 쿠다 사용\n",
    "if torch.cuda.is_available:\n",
    "    model.cuda()\n",
    "print(\"Use Device:\", model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:43:07.596124Z",
     "start_time": "2024-05-28T10:41:14.264145Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9IAbsfJUnEw",
    "outputId": "ed9be902-4890-4ebd-caff-b37bef2700ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/20 loss: 0.20447806457241813\n",
      "epoch:2/20 loss: 0.1993886289688257\n",
      "epoch:3/20 loss: 0.1982472327711818\n",
      "epoch:4/20 loss: 0.19748997802917773\n",
      "epoch:5/20 loss: 0.19661885974826393\n",
      "epoch:6/20 loss: 0.19581917648787026\n",
      "epoch:7/20 loss: 0.19549074376022424\n",
      "epoch:8/20 loss: 0.19530960868348132\n",
      "epoch:9/20 loss: 0.1951195708997957\n",
      "epoch:10/20 loss: 0.19497203106408592\n",
      "epoch:11/20 loss: 0.19484122359490658\n",
      "epoch:12/20 loss: 0.1947005373108518\n",
      "epoch:13/20 loss: 0.1945701030256984\n",
      "epoch:14/20 loss: 0.194445992400358\n",
      "epoch:15/20 loss: 0.1943988683787021\n",
      "epoch:16/20 loss: 0.19421729838455115\n",
      "epoch:17/20 loss: 0.19413333050497286\n",
      "epoch:18/20 loss: 0.19405292842414354\n",
      "epoch:19/20 loss: 0.19397959620743008\n",
      "epoch:20/20 loss: 0.19393013286721575\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataloader, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:52:26.345433Z",
     "start_time": "2024-05-28T10:52:26.291400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u67j9Tw0ZVlF",
    "outputId": "1486b107-53a8-406f-81eb-237dc61d945a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Special Tokens - PAD_IDX: 0, UNK_IDX: 1\n",
      "buy wear a wrong . a . <bos> a bank . "
     ]
    }
   ],
   "source": [
    "model.translate('좋은 아침!', transform=transforms.to_tensor(ko_vocab, tokenizer=ko_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PRACTICE 3: 생성(N-to-M): 한영 번역기 (GPT)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Definition\n",
    "> Encoder와 Decoder가 이해의 편의와 인지 부담을 낮추기 위한 도구였고, 실제로는 Decoder만을 통한 정보 처리도 가능함을 확인 (편견의 극복)\n",
    "\n",
    "> -> 인코더를 통한 정보 압축을 하지 않고 정보를 한번에 다 디코더에 넣는다면 어떨까? 그리고 출력을 한번에 하지 않고 반복한다면 어떨까?\n",
    "\n",
    "![어텐션 스코어](https://slds-lmu.github.io/seminar_nlp_ss20/figures/02-02-attention-and-self-attention-for-nlp/bahdanau-fig3.png)\n",
    "\n",
    "![GPT vs Llama](https://miro.medium.com/v2/resize:fit:1132/1*zdLBI0pShQlgHujAyG_g3g.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스토머 모델을 위해 데이터셋 변환 필요요\n",
    "class AdapterDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, block_size=64):\n",
    "        self.examples = [\n",
    "            tokenizer(\n",
    "                text, truncation=True, max_length=block_size, padding=\"max_length\", return_tensors=\"pt\"\n",
    "            ).input_ids.squeeze() for text in texts if text.strip()\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx], self.examples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gpt2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # reflect pad_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Train\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for inputs, labels in loop:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(input_ids=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Generate text using the Huggingface model.\n",
    "prompt = \"The future of AI is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
