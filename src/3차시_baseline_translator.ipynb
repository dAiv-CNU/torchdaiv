{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-Y63VIHN_d7"
   },
   "source": [
    "\n",
    "# KOR-END 번역 모델 만들기\n",
    "\n",
    "이번 특강에서는 한국어 문장을 영어로 번역하는 시퀀스-투-시퀀스(sequence-to-sequence, seq2seq) 모델을 학습하는 방법을 알아보겠습니다.\n",
    "\n",
    "필요 라이브러리: ``torchtext``, ``spacy`` 를 사용하여 데이터셋을 전처리(preprocess)합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import"
   ],
   "metadata": {
    "id": "NguLhT-UVTbp"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Download Requirements"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade git+https://github.com/dAiv-CNU/torchdaiv.git"
   ],
   "metadata": {
    "id": "kTWvBMDgUWDX",
    "outputId": "26850e09-917f-4862-9707-2c90dbee2896",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download ko_core_news_sm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Library Imports"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:23:27.530041Z",
     "start_time": "2024-05-28T07:23:20.583842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import spacy\n",
    "from torchdaiv import datasets\n",
    "#from torchdaiv.lectures.kor_eng_translator import nn\n",
    "from torchdaiv.lectures.kor_eng_translator.util import vocabulary, transforms\n",
    "\n",
    "from rich.traceback import install\n",
    "#install(show_locals=True)\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Text Preprocess (with Spacy)\n",
    "\n",
    "``torchtext`` 에는 언어 변환 모델을 만들 때 쉽게 사용할 수 있는 데이터셋을 만들기 적합한 다양한 도구가 있습니다.\n",
    "이 예제에서는 가공되지 않은 텍스트 문장(raw text sentence)을 토큰화(tokenize)하고, 어휘집(vocabulary)을 만들고,\n",
    "토큰을 텐서로 숫자화(numericalize)하는 방법을 알아보겠습니다.\n",
    "\n",
    "| (다만, torchtext는 2024년 4월 이후 더 이상 업데이트가 진행되지 않는다는 점에 유의해야 합니다.)\n",
    "\n",
    "아래를 실행하여 Spacy 토크나이저가 쓸 한국어와 영어에 대한 데이터를 다운로드 받습니다."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:23:28.652469Z",
     "start_time": "2024-05-28T07:23:27.531051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# spacy tokenizer 적용\n",
    "ko_core_news_sm = spacy.load(\"ko_core_news_sm\")\n",
    "en_core_web_sm = spacy.load(\"en_core_web_sm\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:23:28.657471Z",
     "start_time": "2024-05-28T07:23:28.653473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ko_tokenizer = lambda x: [token.text for token in ko_core_news_sm(x)]\n",
    "en_tokenizer = lambda x: [token.text for token in en_core_web_sm(x)]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "from spacy.lang.ko.examples import sentences\n",
    "\n",
    "# 작동 확인\n",
    "doc = ko_core_news_sm(sentences[0])\n",
    "print(\"Original:\", doc.text)\n",
    "print(\"Tokenized:\", ko_tokenizer(sentences[0]), end=\"\\n\\n\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\">\", token.text, f\"({token.lemma_}) |\", token.pos_, token.dep_)"
   ],
   "metadata": {
    "id": "q0lVGporOe4n",
    "outputId": "b7406409-3b5e-439b-dc39-bfd8e9b65ae9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-05-28T07:23:28.685129Z",
     "start_time": "2024-05-28T07:23:28.658478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 애플이 영국의 스타트업을 10억 달러에 인수하는 것을 알아보고 있다.\n",
      "Tokenized: ['애플이', '영국의', '스타트업을', '10억', '달러에', '인수하는', '것을', '알아보고', '있다', '.']\n",
      "\n",
      "> 애플이 (애플+이) | NOUN dislocated\n",
      "> 영국의 (영국+의) | PROPN nmod\n",
      "> 스타트업을 (스타트업+을) | NOUN nsubj\n",
      "> 10억 (10+억) | NUM compound\n",
      "> 달러에 (달러+에) | ADV obl\n",
      "> 인수하는 (인수+하+는) | VERB acl\n",
      "> 것을 (것+을) | NOUN obj\n",
      "> 알아보고 (알아보+고) | AUX ROOT\n",
      "> 있다 (있+다) | AUX aux\n",
      "> . (.) | PUNCT punct\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZdlvQ1NN_d9"
   },
   "source": [
    "---\n",
    "## Load Dataset\n",
    "using spacy"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:23:28.724659Z",
     "start_time": "2024-05-28T07:23:28.685129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터셋 로드 - 아무 처리도 하지 않았을 때\n",
    "train_dataset = datasets.AnkiKorEngDataset(\"./data\", split_rate=(0.5, 0.3, 0.2))\n",
    "valid_dataset = datasets.AnkiKorEngDataset(\"./data\", valid=True, split_rate=(0.5, 0.3, 0.2))\n",
    "test_dataset = datasets.AnkiKorEngDataset(\"./data\", test=True, split_rate=(0.5, 0.3, 0.2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed.\n",
      "Dataset loaded. 2945 samples loaded.\n",
      "Extraction completed.\n",
      "Dataset loaded. 1767 samples loaded.\n",
      "Extraction completed.\n",
      "Dataset loaded. 1177 samples loaded.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:23:28.729343Z",
     "start_time": "2024-05-28T07:23:28.725665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터셋 형태 확인\n",
    "sample = list(zip(*train_dataset[0:5]))+list(zip(*train_dataset[500:505]))\n",
    "for i, (kor, eng) in enumerate(sample):\n",
    "    print(i, kor, eng)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 가. Go.\n",
      "1 안녕. Hi.\n",
      "2 뛰어! Run!\n",
      "3 뛰어. Run.\n",
      "4 누구? Who?\n",
      "5 천천히 걸어. Walk slowly.\n",
      "6 내가 틀렸나? Was I wrong?\n",
      "7 우리는 아프다. We are sick.\n",
      "8 우리는 계란을 먹었다. We ate eggs.\n",
      "9 우린 약속했어. We promised.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Vocabulary 생성"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:24:06.865124Z",
     "start_time": "2024-05-28T07:23:28.730347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ko_vocab = vocabulary.build_vocab(raw_dataset=train_dataset.raw_kor, tokenizer=ko_tokenizer)\n",
    "en_vocab = vocabulary.build_vocab(raw_dataset=train_dataset.raw_eng, tokenizer=en_tokenizer)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Convert To Tensor"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:24:42.667732Z",
     "start_time": "2024-05-28T07:24:06.866133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 사전 데이터를 기반으로 데이터셋을 텐서로 변환\n",
    "to_tensor = (\n",
    "    transforms.to_tensor(ko_vocab, tokenizer=ko_tokenizer),\n",
    "    transforms.to_tensor(en_vocab, tokenizer=en_tokenizer)\n",
    ")\n",
    "\n",
    "train_dataset.transform(transform=to_tensor)\n",
    "valid_dataset.transform(transform=to_tensor)\n",
    "test_dataset.transform(transform=to_tensor)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Special Tokens - PAD_IDX: 0, UNK_IDX: 1\n",
      "Using Special Tokens - PAD_IDX: 0, UNK_IDX: 1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:24:42.674862Z",
     "start_time": "2024-05-28T07:24:42.668735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터셋 형태 확인\n",
    "sample = list(zip(*train_dataset[0:5]))+list(zip(*train_dataset[500:505]))\n",
    "for i, (kor, eng) in enumerate(sample):\n",
    "    print(i, kor, eng)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([196,   4,   0,   0,   0,   0,   0,   0,   0,   0]) tensor([452,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "1 tensor([519,   4,   0,   0,   0,   0,   0,   0,   0,   0]) tensor([1902,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "2 tensor([761,  19,   0,   0,   0,   0,   0,   0,   0,   0]) tensor([1343,   44,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "3 tensor([761,   4,   0,   0,   0,   0,   0,   0,   0,   0]) tensor([1343,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "4 tensor([380,   5,   0,   0,   0,   0,   0,   0,   0,   0]) tensor([131,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "5 tensor([ 780, 1579,    4,    0,    0,    0,    0,    0,    0,    0]) tensor([1986,  879,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "6 tensor([  12, 2864,    5,    0,    0,    0,    0,    0,    0,    0]) tensor([754,   5, 246,   8,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "7 tensor([ 23, 793,   4,   0,   0,   0,   0,   0,   0,   0]) tensor([ 40,  27, 497,   4,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "8 tensor([  23, 2865,  633,    4,    0,    0,    0,    0,    0,    0]) tensor([ 40, 549, 458,   4,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "9 tensor([  51, 2866,    4,    0,    0,    0,    0,    0,    0,    0]) tensor([  40, 1987,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Data Loader"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:24:42.686931Z",
     "start_time": "2024-05-28T07:24:42.676393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 배치 크기 결정 후 데이터 로더 생성\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataload = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset)//20)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Definition\n",
    "> RNN/GRU 레이어를 하나만 사용하였던 지난 주차와는 달리, 레이어를 여러 층으로 쌓는 방식의 Encoder와 Decoder 모델을 사용\n",
    "\n",
    "> 인코더는 한국어를 해석하고, 디코더는 영어를 생성하는 방식으로 역할을 나눠서 번역을 수행\n",
    "\n",
    "\n",
    "> 참고사항:\n",
    ">> 아래 예시 모델은 공부하기 쉬운 단순한 모델로 번역에 있어 매우 뛰어난 성능을 보이는 모델은 아닙니다.\n",
    ">> 최신 기술 트렌드는 Transformers를 사용하는 것입니다.\n",
    ">> 혹시 관심이 있다면 [Transformer 레이어](https://pytorch.org/docs/stable/nn.html#transformer-layers)를 사용하는 코드로 변경해서 진행해보기 바랍니다."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:24:42.692929Z",
     "start_time": "2024-05-28T07:24:42.687934Z"
    }
   },
   "cell_type": "code",
   "source": "import torch.nn as nn",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:24:42.704256Z",
     "start_time": "2024-05-28T07:24:42.693932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_vocab = ko_vocab.get_stoi()\n",
    "word_2_idx = en_vocab.get_stoi()\n",
    "idx_2_word = en_vocab.get_itos()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:24:42.711325Z",
     "start_time": "2024-05-28T07:24:42.705260Z"
    }
   },
   "cell_type": "code",
   "source": "len(input_vocab), len(word_2_idx), len(idx_2_word)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7971, 3474, 3474)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J6WQcxktN_d-",
    "ExecuteTime": {
     "end_time": "2024-05-28T07:24:48.786723Z",
     "start_time": "2024-05-28T07:24:48.781458Z"
    }
   },
   "source": [
    "class Encoder(nn.Module):\n",
    "    EMBEDDING_SIZE = 128\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model:nn.Module,\n",
    "            height:int,\n",
    "            hidden:int,\n",
    "            dropout:float = 0.2\n",
    "    ):\n",
    "        self.input_vocab = globals()['input_vocab']\n",
    "        super(Encoder,self).__init__()\n",
    "        self.height = height\n",
    "        self.hidden = hidden\n",
    "        self.embedding = nn.Embedding(len(self.input_vocab), self.EMBEDDING_SIZE)\n",
    "        self.model = model(\n",
    "            self.EMBEDDING_SIZE,\n",
    "            hidden,\n",
    "            num_layers=height,\n",
    "            batch_first = True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = F.relu(x)\n",
    "        _,x = self.model(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:24:50.536373Z",
     "start_time": "2024-05-28T07:24:50.529977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BOS = vocabulary.Token.BOS\n",
    "EOS = vocabulary.Token.EOS\n",
    "PAD = vocabulary.Token.PAD\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    EMBEDDING_SIZE = 128\n",
    "    def __init__(\n",
    "            self,\n",
    "            model:nn.Module,\n",
    "            height:int,\n",
    "            hidden:int,\n",
    "            dropout:float = 0.2\n",
    "    ):\n",
    "        self.idx_2_word = globals()['idx_2_word']\n",
    "        self.word_2_idx = globals()['word_2_idx']\n",
    "        super(Decoder,self).__init__()\n",
    "        self.height = height\n",
    "        self.hidden = hidden\n",
    "        self.embedding = nn.Embedding(len(self.idx_2_word), self.EMBEDDING_SIZE)\n",
    "        self.model = model(\n",
    "            self.EMBEDDING_SIZE,\n",
    "            hidden,\n",
    "            num_layers=height,\n",
    "            batch_first = True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        vocab_size = len(self.idx_2_word)\n",
    "        # self.fc1 = nn.Linear(hidden,vocab_size//2)\n",
    "        # self.fc2 = nn.Linear(vocab_size//2,vocab_size)\n",
    "        self.fc1 = nn.Linear(hidden,hidden//2)\n",
    "        self.fc2 = nn.Linear(hidden//2,vocab_size)\n",
    "\n",
    "    def forward(self,x,cv=None):\n",
    "        x = self.embedding(x)\n",
    "        x = F.relu(x)\n",
    "        output,last_hidden = self.model(x,cv)\n",
    "        x = F.relu(self.fc1(output))\n",
    "        x = self.fc2(x)\n",
    "        return x,last_hidden\n",
    "\n",
    "    def generate(self,cv):\n",
    "        word = BOS\n",
    "        cnt = 0\n",
    "        while word != EOS:\n",
    "            if cnt > 10:\n",
    "                break\n",
    "            x = torch.tensor(self.word_2_idx[word]).unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "            x,cv = self(x,cv)\n",
    "            _,x = torch.max(x.view(-1,len(self.idx_2_word)),dim=1)\n",
    "            word = self.idx_2_word[x.item()]\n",
    "            print(word,end=\" \")\n",
    "            cnt += 1"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:24:51.111997Z",
     "start_time": "2024-05-28T07:24:51.097242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "height = 2\n",
    "hidden = 64\n",
    "encoder = Encoder(nn.GRU, height=height, hidden=hidden, dropout=0.3)\n",
    "decoder = Decoder(nn.GRU, height=height, hidden=hidden, dropout=0.3)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:26:01.409681Z",
     "start_time": "2024-05-28T07:26:01.404398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_loop(encoder,decoder,dataset,epochs,lr,encoder_optimizer,decoder_optimizer,criterion):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    decoder.to('cuda')\n",
    "    encoder.to('cuda')\n",
    "    for i,epoch in enumerate(range(epochs)):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for step,batch in enumerate(dataset):\n",
    "            srcs, tgts = batch\n",
    "            encoder_input = torch.tensor([[input_vocab[token] for token in src] for src in srcs]).to('cuda')\n",
    "            decoder_input = torch.tensor([[word_2_idx[token] for token in [BOS] + tgt] for tgt in tgts]).to('cuda')\n",
    "            decoder_label = torch.tensor([[word_2_idx[token] for token in tgt + [EOS]] for tgt in tgts]).to('cuda')\n",
    "\n",
    "            context_vector = encoder(encoder_input)\n",
    "            x,_ = decoder(decoder_input,context_vector)\n",
    "\n",
    "            decoder_label = torch.nn.functional.one_hot(decoder_label, num_classes=11).float()\n",
    "            loss = criterion(x,decoder_label)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.step()\n",
    "            decoder_optimizer.zero_grad()\n",
    "        print(f\"epoch:{i+1}/{epochs} loss: {running_loss}\")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4EYOmC9N_d-"
   },
   "source": [
    "참고 : 언어 번역의 성능 점수를 기록하려면, ``nn.CrossEntropyLoss`` 함수가 단순한\n",
    "패딩을 추가하는 부분을 무시할 수 있도록 해당 색인들을 알려줘야 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Diz2_otnN_d-",
    "ExecuteTime": {
     "end_time": "2024-05-28T07:26:03.398883Z",
     "start_time": "2024-05-28T07:26:03.394275Z"
    }
   },
   "source": [
    "lr = 1e-4\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(),lr)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(),lr)\n",
    "\n",
    "PAD_IDX = ko_vocab[PAD]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:26:04.628320Z",
     "start_time": "2024-05-28T07:26:03.754446Z"
    }
   },
   "cell_type": "code",
   "source": "train_loop(encoder,decoder,train_dataset,100,lr,encoder_optimizer,decoder_optimizer,criterion)",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43mencoder_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdecoder_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[25], line 11\u001B[0m, in \u001B[0;36mtrain_loop\u001B[1;34m(encoder, decoder, dataset, epochs, lr, encoder_optimizer, decoder_optimizer, criterion)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step,batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataset):\n\u001B[0;32m     10\u001B[0m     srcs, tgts \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m---> 11\u001B[0m     encoder_input \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43minput_vocab\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msrcs\u001B[49m\u001B[43m]\u001B[49m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     12\u001B[0m     decoder_input \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([[word_2_idx[token] \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m [BOS] \u001B[38;5;241m+\u001B[39m tgt] \u001B[38;5;28;01mfor\u001B[39;00m tgt \u001B[38;5;129;01min\u001B[39;00m tgts])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     13\u001B[0m     decoder_label \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([[word_2_idx[token] \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m tgt \u001B[38;5;241m+\u001B[39m [EOS]] \u001B[38;5;28;01mfor\u001B[39;00m tgt \u001B[38;5;129;01min\u001B[39;00m tgts])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[25], line 11\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step,batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataset):\n\u001B[0;32m     10\u001B[0m     srcs, tgts \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m---> 11\u001B[0m     encoder_input \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[43m[\u001B[49m\u001B[43minput_vocab\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m src \u001B[38;5;129;01min\u001B[39;00m srcs])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     12\u001B[0m     decoder_input \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([[word_2_idx[token] \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m [BOS] \u001B[38;5;241m+\u001B[39m tgt] \u001B[38;5;28;01mfor\u001B[39;00m tgt \u001B[38;5;129;01min\u001B[39;00m tgts])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     13\u001B[0m     decoder_label \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([[word_2_idx[token] \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m tgt \u001B[38;5;241m+\u001B[39m [EOS]] \u001B[38;5;28;01mfor\u001B[39;00m tgt \u001B[38;5;129;01min\u001B[39;00m tgts])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\torch\\_tensor.py:1047\u001B[0m, in \u001B[0;36mTensor.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1037\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1038\u001B[0m     \u001B[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001B[39;00m\n\u001B[0;32m   1039\u001B[0m     \u001B[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001B[39;00m\n\u001B[0;32m   1045\u001B[0m     \u001B[38;5;66;03m# See gh-54457\u001B[39;00m\n\u001B[0;32m   1046\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 1047\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miteration over a 0-d tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1048\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_get_tracing_state():\n\u001B[0;32m   1049\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1050\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1051\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing a tensor of different shape won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt change the number of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1055\u001B[0m             stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m   1056\u001B[0m         )\n",
      "\u001B[1;31mTypeError\u001B[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwL84unhN_d-"
   },
   "source": [
    "마지막으로 이 모델을 훈련하고 평가합니다 :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qIM8QMi_N_d_"
   },
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: torch.utils.data.DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H-qvXAJN_d_"
   },
   "source": [
    "## 다음 단계\n",
    "\n",
    "- ``torchtext`` 를 사용한 Ben Trevett의 튜토리얼을 [이곳](https://github.com/bentrevett/)_ 에서 확인할 수 있습니다.\n",
    "- ``nn.Transformer`` 와 ``torchtext`` 의 다른 기능들을 이용한 다음 단어 예측을 통한 언어 모델링 튜토리얼을 살펴보세요.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
